{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "@Author   :   Corley Tang\n",
    "@contact  :   cutercorleytd@gmail.com\n",
    "@Github   :   https://github.com/corleytd\n",
    "@Time     :   2023-01-10 15:03\n",
    "@Project  :   Hands-on Deep Learning with PyTorch-tensor_index_slice_merge\n",
    "张量的索引、分片、合并以及维度调整\n",
    "'''\n",
    "\n",
    "# 导入所需的库\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.张量的符号索引"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.一维张量索引\n",
    "t1 = torch.arange(0, 30, 3)\n",
    "t1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(0), tensor(9))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0], t1[3]  # 张量索引的结果还是张量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 6,  9, 12])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切片\n",
    "t1[2:5]  # 左包含右不包含"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 3,  9, 15, 21])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切片，带索引间隔\n",
    "# t1[8:1:-1]  # ValueError，在张量的索引中，step必须大于0\n",
    "t1[1:8:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([ 3,  9, 15, 21, 27]), tensor([ 0,  6, 12, 18]))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1::2], t1[:8:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  3,  4,  5,  6],\n        [ 7,  8,  9, 10, 11, 12],\n        [13, 14, 15, 16, 17, 18],\n        [19, 20, 21, 22, 23, 24]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.二维张量索引\n",
    "t2 = torch.arange(1, 25).reshape(4, 6)\n",
    "t2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(9),\n tensor([ 7,  9, 11]),\n tensor([ 7,  9, 11]),\n tensor([ 2, 14]),\n tensor([[ 1,  3,  5],\n         [13, 15, 17]]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[1, 2], t2[1, ::2], t2[1, [0, 2, 4]], t2[[0, 2], 1], t2[::2, ::2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[  1,   2,   3,   4,   5,   6],\n         [  7,   8,   9,  10,  11,  12],\n         [ 13,  14,  15,  16,  17,  18],\n         [ 19,  20,  21,  22,  23,  24],\n         [ 25,  26,  27,  28,  29,  30]],\n\n        [[ 31,  32,  33,  34,  35,  36],\n         [ 37,  38,  39,  40,  41,  42],\n         [ 43,  44,  45,  46,  47,  48],\n         [ 49,  50,  51,  52,  53,  54],\n         [ 55,  56,  57,  58,  59,  60]],\n\n        [[ 61,  62,  63,  64,  65,  66],\n         [ 67,  68,  69,  70,  71,  72],\n         [ 73,  74,  75,  76,  77,  78],\n         [ 79,  80,  81,  82,  83,  84],\n         [ 85,  86,  87,  88,  89,  90]],\n\n        [[ 91,  92,  93,  94,  95,  96],\n         [ 97,  98,  99, 100, 101, 102],\n         [103, 104, 105, 106, 107, 108],\n         [109, 110, 111, 112, 113, 114],\n         [115, 116, 117, 118, 119, 120]]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.三维张量的索引\n",
    "t3 = torch.arange(1, 121).reshape(4, 5, 6)\n",
    "t3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(83),\n tensor([[62, 64, 66],\n         [74, 76, 78],\n         [86, 88, 90]]),\n tensor([[[ 9, 11],\n          [21, 23]],\n \n         [[69, 71],\n          [81, 83]]]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[2, 3, 4], t3[2, ::2, 1::2], t3[::2, 1::2, 2::2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.张量的函数索引"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, tensor([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27]))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.ndim, t1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 9])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用index_select函数进行索引\n",
    "indices = torch.tensor([1, 3])\n",
    "torch.index_select(t1, 0, indices)  # 也可以用torch.index_select(t1, -1, indices)，等价于t1[[1, 2, 5]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4, 6]),\n tensor([[ 1,  2,  3,  4,  5,  6],\n         [ 7,  8,  9, 10, 11, 12],\n         [13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 23, 24]]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape, t2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 7,  8,  9, 10, 11, 12],\n         [19, 20, 21, 22, 23, 24]]),\n tensor([[ 2,  4],\n         [ 8, 10],\n         [14, 16],\n         [20, 22]]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t2, 0, indices), torch.index_select(t2, 1, indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.torch.view()方法\n",
    "PyTorch中的view()方法会返回一个类似视图的结果，该结果和原张量对象共享一块数据存储空间，并且通过view()方法，还可以改变对象结构，生成一个不同结构，但共享一个存储空间的张量。当然，共享一个存储空间，也就代表者是**浅拷贝**的关系，修改其中一个，另一个也会同步进行更改。视图的核心作用就是节省空间，张量的切分和合并得到的结果都是视图，而不是生成的新的对象。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1,  2,  3,  4,  5,  6],\n         [ 7,  8,  9, 10, 11, 12],\n         [13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 23, 24]]),\n tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n         [ 9, 10, 11, 12, 13, 14, 15, 16],\n         [17, 18, 19, 20, 21, 22, 23, 24]]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_view_2 = t2.view(3, 8)  # 构建一个数据相同、但形状不同的“视图”，两者指向同一个对象\n",
    "t2, t2_view_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1,  2,  3,  4,  5,  6],\n         [20,  8, 20, 10, 20, 12],\n         [13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 23, 24]]),\n tensor([[ 1,  2,  3,  4,  5,  6, 20,  8],\n         [20, 10, 20, 12, 13, 14, 15, 16],\n         [17, 18, 19, 20, 21, 22, 23, 24]]))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[1, ::2] = 20  # 两者会同步发生改变\n",
    "t2, t2_view_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1,  2,  3,  4,  5,  6],\n         [20,  8, 20, 10, 20, 12],\n         [13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 23, 24]]),\n tensor([[[ 1,  2,  3,  4],\n          [ 5,  6, 20,  8]],\n \n         [[20, 10, 20, 12],\n          [13, 14, 15, 16]],\n \n         [[17, 18, 19, 20],\n          [21, 22, 23, 24]]]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_view_3 = t2.view(3, 2, 4)\n",
    "t2, t2_view_3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1,  2,  3,  4,  5,  6],\n         [30, 30, 20, 10, 20, 12],\n         [13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 30, 30]]),\n tensor([[[ 1,  2,  3,  4],\n          [ 5,  6, 30, 30]],\n \n         [[20, 10, 20, 12],\n          [13, 14, 15, 16]],\n \n         [[17, 18, 19, 20],\n          [21, 22, 30, 30]]]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_view_3[::2, 1, 2:] = 30\n",
    "t2, t2_view_3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.张量的分片函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1,  2,  3,  4,  5,  6],\n         [30, 30, 20, 10, 20, 12],\n         [13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 30, 30]]),\n (tensor([[1, 2, 3, 4, 5, 6]]),\n  tensor([[30, 30, 20, 10, 20, 12]]),\n  tensor([[13, 14, 15, 16, 17, 18]]),\n  tensor([[19, 20, 21, 22, 30, 30]])),\n tensor([1, 2, 3, 4, 5, 6]))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1.分块——chunk函数：按照某维度，对张量进行均匀切分，并且返回结果是原张量的视图，不改变维度\n",
    "t2_chunked = torch.chunk(t2, 4, dim=0)\n",
    "t2, t2_chunked, t2_chunked[0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[10,  2, 10,  4, 10,  6],\n         [30, 30, 20, 10, 20, 12],\n         [13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 30, 30]]),\n (tensor([[10,  2, 10,  4, 10,  6]]),\n  tensor([[30, 30, 20, 10, 20, 12]]),\n  tensor([[13, 14, 15, 16, 17, 18]]),\n  tensor([[19, 20, 21, 22, 30, 30]])),\n tensor([10,  2, 10,  4, 10,  6]))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_chunked[0][0][::2] = 10  # 两者会同步发生改变\n",
    "t2, t2_chunked, t2_chunked[0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "((tensor([[10,  2, 10,  4, 10,  6],\n          [30, 30, 20, 10, 20, 12]]),\n  tensor([[13, 14, 15, 16, 17, 18],\n          [19, 20, 21, 22, 30, 30]])),\n (tensor([[10],\n          [30],\n          [13],\n          [19]]),\n  tensor([[ 2],\n          [30],\n          [14],\n          [20]]),\n  tensor([[10],\n          [20],\n          [15],\n          [21]]),\n  tensor([[ 4],\n          [10],\n          [16],\n          [22]]),\n  tensor([[10],\n          [20],\n          [17],\n          [30]]),\n  tensor([[ 6],\n          [12],\n          [18],\n          [30]])))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(t2, 3, dim=0), torch.chunk(t2, 7, dim=1)  # 当原张量不能均分时，chunk不会报错，但会返回其他均分的结果，即次一级均分结果或者非等分结果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[10,  2, 10,  4, 10,  6],\n         [30, 30, 20, 10, 20, 12]]),\n tensor([[13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 30, 30]]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.拆分——split函数：split既能进行均分，也能进行自定义切分，返回结果也是view\n",
    "# 均分\n",
    "t2_splitted = torch.split(t2, 2, 0)  # 第二个参数只输入一个数时表示均分\n",
    "t2_splitted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "((tensor([[10,  2, 10,  4, 10,  6],\n          [30, 30, 20, 10, 20, 12],\n          [13, 14, 15, 16, 17, 18]]),\n  tensor([[19, 20, 21, 22, 30, 30]])),\n (tensor([[10,  2, 10,  4, 10,  6]]),\n  tensor([[30, 30, 20, 10, 20, 12],\n          [13, 14, 15, 16, 17, 18]]),\n  tensor([[19, 20, 21, 22, 30, 30]])),\n (tensor([[10],\n          [30],\n          [13],\n          [19]]),\n  tensor([[ 2, 10],\n          [30, 20],\n          [14, 15],\n          [20, 21]]),\n  tensor([[ 4],\n          [10],\n          [16],\n          [22]]),\n  tensor([[10,  6],\n          [20, 12],\n          [17, 18],\n          [30, 30]])))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.split(t2, [3, 2], 0)  # RuntimeError，当第二个参数位输入一个序列时，序列的各数值之和必须等于对应维度下形状分量的取值\n",
    "torch.split(t2, [3, 1], 0), torch.split(t2, [1, 2, 1], 0), torch.split(t2, [1, 2, 1, 2], 1)  # 第二个参数输入一个序列时表示按照序列数值进行切分"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[10,  2, 10,  4, 10,  6],\n         [30, 30, 40, 10, 40, 12],\n         [13, 14, 15, 16, 17, 18],\n         [19, 20, 21, 22, 30, 30]]),\n (tensor([[10,  2, 10,  4, 10,  6],\n          [30, 30, 40, 10, 40, 12]]),\n  tensor([[13, 14, 15, 16, 17, 18],\n          [19, 20, 21, 22, 30, 30]])))"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_splitted[0][1:, 2::2] = 40\n",
    "t2, t2_splitted  # view进行修改，原对象同步修改"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.张量的合并操作\n",
    "张量的合并操作类似于列表的追加元素，可以拼接、也可以堆叠。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 0., 0.],\n         [0., 0., 0.]]),\n tensor([[1., 1., 1.],\n         [1., 1., 1.]]),\n tensor([[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]))"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.拼接——cat函数：实现张量的拼接\n",
    "a = torch.zeros(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "c = torch.zeros(3, 3)\n",
    "a, b, c"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [1., 1., 1.],\n        [1., 1., 1.]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b])  # 按行进行拼接，dim参数默认为0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1.]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat([a, c], dim=1)  # RuntimeError，对应维度形状不匹配无法进行拼接\n",
    "torch.cat([a, b], dim=1)  # 按列进行拼接"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 3]), torch.Size([2, 3]), torch.Size([3, 3]))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.堆叠——stack函数：堆叠不是将元素拆分重装，而是简单地将各参与堆叠的对象分装到一个更高维度的张量\n",
    "a.shape, b.shape, c.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 2, 3]),\n tensor([[[0., 0., 0.],\n          [0., 0., 0.]],\n \n         [[1., 1., 1.],\n          [1., 1., 1.]]]))"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.stack([a, c])  # RuntimeError，被堆叠的张量形状应该一致\n",
    "ab_stacked = torch.stack([a, b])  # 堆叠之后，生成1个三维张量\n",
    "ab_stacked.shape, ab_stacked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "二者区别：\n",
    "- 拼接之后维度不变，堆叠之后维度升高\n",
    "- 拼接是把一个个元素单独提取出来之后再放到二维张量中，而堆叠则是直接将两个二维张量封装到一个三维张量中\n",
    "- 堆叠的要求更高，参与堆叠的张量必须形状完全相同\n",
    "## 6.张量维度变换\n",
    "通过reshape方法， 能够灵活调整张量的形状，而在实际操作张量进行计算时，往往需要另外进行降维和升维的操作：\n",
    "- 当我们需要除去不必要的维度时，可以使用squeeze函数\n",
    "- 需要手动升维时，则可采用unsqueeze函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(4,\n torch.Size([1, 1, 3, 1]),\n tensor([[[[1.],\n           [1.],\n           [1.]]]]))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.ones(1, 1, 3, 1)  # 1个包含1个三维的四维张量，三维张量只包含1个三行一列的二维张量\n",
    "t4.ndim, t4.shape, t4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3]), tensor([1., 1., 1.]))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.squeeze函数：删除不必要的维度\n",
    "t4_squeezed = torch.squeeze(t4)  # 去除为1的维度，等价于t4.squeeze()\n",
    "t4_squeezed.shape, t4_squeezed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(6,\n torch.Size([1, 1, 3, 2, 1, 2]),\n tensor([[[[[[1., 1.]],\n \n            [[1., 1.]]],\n \n \n           [[[1., 1.]],\n \n            [[1., 1.]]],\n \n \n           [[[1., 1.]],\n \n            [[1., 1.]]]]]]))"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5 = torch.ones(1, 1, 3, 2, 1, 2)\n",
    "t5.ndim, t5.shape, t5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 2, 2]),\n tensor([[[1., 1.],\n          [1., 1.]],\n \n         [[1., 1.],\n          [1., 1.]],\n \n         [[1., 1.],\n          [1., 1.]]]))"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_squeezed = torch.squeeze(t5)\n",
    "t5_squeezed.shape, t5_squeezed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 2, 1, 3]),\n tensor([[[[5, 5, 5]],\n \n          [[5, 5, 5]]]]))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.unsqueeze函数：手动升维\n",
    "t6 = torch.full((1, 2, 1, 3), 5)\n",
    "t6.shape, t6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 1, 2, 1, 3]),\n tensor([[[[[5, 5, 5]],\n \n           [[5, 5, 5]]]]]))"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t6_unsqueezed = torch.unsqueeze(t6, dim=0)  # 在指定维度上升维，等价于t6.unsqueeze(0)\n",
    "t6_unsqueezed.shape, t6_unsqueezed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 2, 1, 1, 3]), torch.Size([1, 2, 1, 3, 1]))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t6, dim=2).shape, torch.unsqueeze(t6, dim=4).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
